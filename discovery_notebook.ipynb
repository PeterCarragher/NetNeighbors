{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç News Source Discovery Using CommonCrawl Webgraph\n",
    "\n",
    "Discover related domains using link topology analysis from the CommonCrawl web graph.\n",
    "\n",
    "**Based on:**\n",
    "- Carragher, P., Williams, E. M., & Carley, K. M. (2024). *Detection and Discovery of Misinformation Sources using Attributed Webgraphs*. ICWSM 2024. [Paper](https://arxiv.org/abs/2401.02379)\n",
    "- Carragher, P., Williams, E. M., Spezzano, F., & Carley, K. M. (2025). *Misinformation Resilient Search Rankings with Attributed Webgraphs*. ACM TIST.\n",
    "\n",
    "**Dataset:**\n",
    "- CommonCrawl webgraph (Nov-Dec 2024, Jan 2025)\n",
    "- 93.9M domains, 1.6B edges\n",
    "- Domain-level aggregation\n",
    "\n",
    "**What this notebook does:**\n",
    "Given a list of seed domains, discovers other domains that are connected via backlinks or outlinks in the CommonCrawl web graph.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Setup Instructions\n",
    "\n",
    "**‚è±Ô∏è Time: ~15 minutes (first time only)**\n",
    "\n",
    "### Step 1: Enable High-RAM Runtime (REQUIRED)\n",
    "\n",
    "1. Click **Runtime** ‚Üí **Change runtime type**\n",
    "2. Set **Runtime shape** to **High-RAM** ‚ö†Ô∏è\n",
    "3. Set **Hardware accelerator** to **GPU** (optional, for faster processing)\n",
    "4. Click **Save**\n",
    "\n",
    "*Why? The CommonCrawl webgraph is 22.5GB and requires >40GB RAM to process.*\n",
    "\n",
    "### Step 2: (Optional) Mount Google Drive\n",
    "\n",
    "**Recommended!** This caches the 22.5GB webgraph so you don't re-download it every session.\n",
    "\n",
    "Run the \"Mount Google Drive\" cell below and follow the prompts.\n",
    "\n",
    "### Step 3: Run Setup Cells (One-Time)\n",
    "\n",
    "**‚ñ∂Ô∏è Click Run on each setup cell in order** (Cells 3-8)\n",
    "\n",
    "Progress bars will show download status. Wait for each cell to complete before running the next.\n",
    "\n",
    "### Step 4: Use the Discovery Form\n",
    "\n",
    "Scroll down to **Section 3: Discovery Interface** and interact with the form!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup (Run Once)\n",
    "\n",
    "### Check Available RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# Check available RAM\n",
    "ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "print(f\"Available RAM: {ram_gb:.1f} GB\")\n",
    "\n",
    "if ram_gb < 20:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: You need Colab Pro for this notebook!\")\n",
    "    print(\"   Required: 20GB+ RAM\")\n",
    "    print(f\"   You have: {ram_gb:.1f} GB\")\n",
    "    print(\"\\n   Please enable High-RAM runtime:\")\n",
    "    print(\"   Runtime ‚Üí Change runtime type ‚Üí Runtime shape: High-RAM\")\n",
    "    raise Exception(\"Insufficient RAM. Please upgrade runtime.\")\n",
    "else:\n",
    "    print(\"‚úÖ Sufficient RAM available\")\n",
    "    print(\"\\nYou can proceed with setup!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount Google Drive (Optional but Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Ask user if they want to mount Drive\n",
    "print(\"Mount Google Drive to cache webgraph between sessions?\")\n",
    "print(\"This saves ~15 minutes on future runs.\")\n",
    "print(\"\")\n",
    "mount_choice = input(\"Mount Google Drive? (yes/no): \").lower().strip()\n",
    "\n",
    "if mount_choice in ['yes', 'y']:\n",
    "    drive.mount('/content/drive')\n",
    "    WEBGRAPH_DIR = '/content/drive/MyDrive/Colab_Data/webgraph'\n",
    "    print(f\"\\n‚úÖ Webgraph will be cached in: {WEBGRAPH_DIR}\")\n",
    "    print(\"This will persist across sessions!\")\n",
    "else:\n",
    "    WEBGRAPH_DIR = '/content/webgraph'\n",
    "    print(f\"\\n‚ö†Ô∏è Webgraph will be downloaded each session (~15 min)\")\n",
    "    print(f\"Stored temporarily in: {WEBGRAPH_DIR}\")\n",
    "\n",
    "# Create directory\n",
    "os.makedirs(WEBGRAPH_DIR, exist_ok=True)\n",
    "print(f\"\\nDirectory created: {WEBGRAPH_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Java 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"Installing Java 17...\"\n",
    "apt-get update -qq > /dev/null 2>&1\n",
    "apt-get install -y -qq openjdk-17-jdk-headless maven > /dev/null 2>&1\n",
    "\n",
    "echo \"‚úÖ Java installation complete\"\n",
    "java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download cc-webgraph Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Clone and build cc-webgraph\n",
    "if [ ! -d \"cc-webgraph\" ]; then\n",
    "    echo \"Cloning cc-webgraph repository...\"\n",
    "    git clone https://github.com/commoncrawl/cc-webgraph.git > /dev/null 2>&1\n",
    "    \n",
    "    echo \"Building cc-webgraph (this may take 1-2 minutes)...\"\n",
    "    cd cc-webgraph\n",
    "    mvn clean package -DskipTests -q\n",
    "    \n",
    "    echo \"‚úÖ cc-webgraph built successfully\"\n",
    "else\n",
    "    echo \"‚úÖ cc-webgraph already exists\"\n",
    "fi\n",
    "\n",
    "# Verify JAR file exists\n",
    "if [ -f \"cc-webgraph/target/cc-webgraph-0.1-SNAPSHOT-jar-with-dependencies.jar\" ]; then\n",
    "    echo \"‚úÖ JAR file found\"\n",
    "else\n",
    "    echo \"‚ùå JAR file not found. Build may have failed.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download CommonCrawl Webgraph Data (~10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import urllib.request\n",
    "\n",
    "VERSION = \"cc-main-2025-26-nov-dec-jan\"\n",
    "BASE_URL = f\"https://data.commoncrawl.org/projects/hyperlinkgraph/{VERSION}/domain\"\n",
    "\n",
    "files_to_download = [\n",
    "    f\"{VERSION}-domain-vertices.txt.gz\",\n",
    "    f\"{VERSION}-domain-edges.txt.gz\"\n",
    "]\n",
    "\n",
    "def download_with_progress(url, dest_path):\n",
    "    \"\"\"Download file with progress bar\"\"\"\n",
    "    if os.path.exists(dest_path):\n",
    "        print(f\"‚úÖ Already downloaded: {os.path.basename(dest_path)}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Downloading: {os.path.basename(dest_path)}\")\n",
    "    \n",
    "    def progress_hook(pbar):\n",
    "        def update(block_num, block_size, total_size):\n",
    "            if total_size > 0:\n",
    "                pbar.total = total_size\n",
    "                pbar.update(block_size)\n",
    "        return update\n",
    "    \n",
    "    with tqdm(unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "        urllib.request.urlretrieve(url, dest_path, reporthook=progress_hook(pbar))\n",
    "    \n",
    "    print(f\"‚úÖ Downloaded: {os.path.basename(dest_path)}\")\n",
    "\n",
    "print(\"Downloading CommonCrawl webgraph (22.5GB total)...\")\n",
    "print(f\"Destination: {WEBGRAPH_DIR}\\n\")\n",
    "\n",
    "for filename in files_to_download:\n",
    "    url = f\"{BASE_URL}/{filename}\"\n",
    "    dest = os.path.join(WEBGRAPH_DIR, filename)\n",
    "    download_with_progress(url, dest)\n",
    "\n",
    "print(\"\\n‚úÖ All files downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph Structures (~2 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$WEBGRAPH_DIR\"\n",
    "WEBGRAPH_DIR=$1\n",
    "VERSION=\"cc-main-2025-26-nov-dec-jan\"\n",
    "\n",
    "cd /content/cc-webgraph\n",
    "\n",
    "VERTICES=\"${WEBGRAPH_DIR}/${VERSION}-domain-vertices.txt.gz\"\n",
    "EDGES=\"${WEBGRAPH_DIR}/${VERSION}-domain-edges.txt.gz\"\n",
    "OUTPUT=\"${WEBGRAPH_DIR}/${VERSION}-domain\"\n",
    "\n",
    "# Check if already built\n",
    "if [ -f \"${OUTPUT}.graph\" ]; then\n",
    "    echo \"‚úÖ Graph structures already built\"\n",
    "    exit 0\n",
    "fi\n",
    "\n",
    "echo \"Building graph structures (this takes ~2 minutes)...\"\n",
    "echo \"This converts the edge list into an efficient queryable format.\"\n",
    "echo \"\"\n",
    "\n",
    "./src/script/webgraph_ranking/process_webgraph.sh \\\n",
    "    preference_up \\\n",
    "    \"$VERTICES\" \\\n",
    "    \"$EDGES\" \\\n",
    "    \"$OUTPUT\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úÖ Graph structures built successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import gzip\n",
    "\n",
    "print(\"Verifying installation...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check Java\n",
    "try:\n",
    "    result = subprocess.run(['java', '-version'], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        version_line = result.stderr.split('\\n')[0]\n",
    "        print(f\"‚úÖ Java: {version_line}\")\n",
    "    else:\n",
    "        print(\"‚ùå Java: Not working properly\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Java: Error - {e}\")\n",
    "\n",
    "# Check cc-webgraph JAR\n",
    "jar_path = \"/content/cc-webgraph/target/cc-webgraph-0.1-SNAPSHOT-jar-with-dependencies.jar\"\n",
    "if os.path.exists(jar_path):\n",
    "    size_mb = os.path.getsize(jar_path) / (1024 * 1024)\n",
    "    print(f\"‚úÖ cc-webgraph JAR: Found ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(f\"‚ùå cc-webgraph JAR: Not found at {jar_path}\")\n",
    "\n",
    "# Check webgraph data\n",
    "VERSION = \"cc-main-2025-26-nov-dec-jan\"\n",
    "vertices_file = os.path.join(WEBGRAPH_DIR, f\"{VERSION}-domain-vertices.txt.gz\")\n",
    "edges_file = os.path.join(WEBGRAPH_DIR, f\"{VERSION}-domain-edges.txt.gz\")\n",
    "graph_file = os.path.join(WEBGRAPH_DIR, f\"{VERSION}-domain.graph\")\n",
    "\n",
    "if os.path.exists(vertices_file):\n",
    "    size_mb = os.path.getsize(vertices_file) / (1024 * 1024)\n",
    "    print(f\"‚úÖ Vertices file: Found ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    # Count domains\n",
    "    print(\"   Counting domains...\")\n",
    "    try:\n",
    "        with gzip.open(vertices_file, 'rt', encoding='utf-8') as f:\n",
    "            num_domains = sum(1 for _ in f)\n",
    "        print(f\"   ‚Üí {num_domains:,} domains in webgraph\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚Üí Could not count domains: {e}\")\n",
    "else:\n",
    "    print(f\"‚ùå Vertices file: Not found\")\n",
    "\n",
    "if os.path.exists(edges_file):\n",
    "    size_mb = os.path.getsize(edges_file) / (1024 * 1024)\n",
    "    print(f\"‚úÖ Edges file: Found ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(f\"‚ùå Edges file: Not found\")\n",
    "\n",
    "if os.path.exists(graph_file):\n",
    "    size_mb = os.path.getsize(graph_file) / (1024 * 1024)\n",
    "    print(f\"‚úÖ Graph structure: Built ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(f\"‚ùå Graph structure: Not built\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Final verdict\n",
    "all_good = all([\n",
    "    os.path.exists(jar_path),\n",
    "    os.path.exists(vertices_file),\n",
    "    os.path.exists(edges_file),\n",
    "    os.path.exists(graph_file)\n",
    "])\n",
    "\n",
    "if all_good:\n",
    "    print(\"\\nüéâ Setup complete! Ready to discover domains.\")\n",
    "    print(\"\\nScroll down to Section 2 to use the discovery interface.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Setup incomplete. Please re-run failed cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Helper Functions\n",
    "\n",
    "These cells define the discovery functionality. You don't need to modify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import os\n",
    "import gzip\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class WebgraphDiscovery:\n",
    "    \"\"\"\n",
    "    Wrapper class for running webgraph discovery using cc-webgraph tools.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, webgraph_dir: str, version: str):\n",
    "        self.webgraph_dir = webgraph_dir\n",
    "        self.version = version\n",
    "        self.jar_path = \"/content/cc-webgraph/target/cc-webgraph-0.1-SNAPSHOT-jar-with-dependencies.jar\"\n",
    "        self.graph_base = os.path.join(webgraph_dir, f\"{version}-domain\")\n",
    "        self.vertices_file = os.path.join(webgraph_dir, f\"{version}-domain-vertices.txt.gz\")\n",
    "        \n",
    "        # Load domain mapping (for validation)\n",
    "        self._domain_set = None\n",
    "        \n",
    "    def _load_domain_set(self) -> set:\n",
    "        \"\"\"Load set of all domains in webgraph (for validation)\"\"\"\n",
    "        if self._domain_set is not None:\n",
    "            return self._domain_set\n",
    "        \n",
    "        print(\"Loading domain list (one-time, ~30 seconds)...\")\n",
    "        domains = set()\n",
    "        with gzip.open(self.vertices_file, 'rt', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 2:\n",
    "                    reversed_domain = parts[1]\n",
    "                    # Convert back to normal notation\n",
    "                    domain = '.'.join(reversed(reversed_domain.split('.')))\n",
    "                    domains.add(domain)\n",
    "        \n",
    "        self._domain_set = domains\n",
    "        print(f\"‚úÖ Loaded {len(domains):,} domains\")\n",
    "        return domains\n",
    "    \n",
    "    def validate_seeds(self, seed_domains: List[str]) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Validate which seed domains exist in webgraph\"\"\"\n",
    "        domain_set = self._load_domain_set()\n",
    "        \n",
    "        found = []\n",
    "        not_found = []\n",
    "        \n",
    "        for domain in seed_domains:\n",
    "            domain_clean = domain.strip().lower()\n",
    "            if domain_clean in domain_set:\n",
    "                found.append(domain_clean)\n",
    "            else:\n",
    "                not_found.append(domain_clean)\n",
    "        \n",
    "        return found, not_found\n",
    "    \n",
    "    def discover(self, \n",
    "                 seed_domains: List[str], \n",
    "                 min_connections: int,\n",
    "                 direction: str = 'backlinks') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run discovery algorithm using cc-webgraph tools.\n",
    "        \n",
    "        This creates a simple Java program inline and executes it.\n",
    "        The program uses WebGraph's BVGraph to query neighbors efficiently.\n",
    "        \n",
    "        For backlinks: uses the transpose graph (-t.graph) where successors = predecessors\n",
    "        For outlinks: uses the regular graph (.graph) where successors = outlinks\n",
    "        \"\"\"\n",
    "        # Write seeds to file (in normal notation)\n",
    "        seeds_file = '/content/seeds.txt'\n",
    "        with open(seeds_file, 'w') as f:\n",
    "            for domain in seed_domains:\n",
    "                f.write(domain.strip().lower() + '\\n')\n",
    "        \n",
    "        # Create a simple discovery Java program\n",
    "        # This uses cc-webgraph's existing classes\n",
    "        java_code = self._generate_discovery_java_code(\n",
    "            seeds_file, min_connections, direction\n",
    "        )\n",
    "        \n",
    "        # Write Java code\n",
    "        java_file = '/content/DiscoveryRunner.java'\n",
    "        with open(java_file, 'w') as f:\n",
    "            f.write(java_code)\n",
    "        \n",
    "        # Compile and run\n",
    "        print(\"\\nRunning discovery algorithm...\")\n",
    "        print(f\"Direction: {direction}\")\n",
    "        print(f\"Min connections: {min_connections}\")\n",
    "        print(f\"Seed domains: {len(seed_domains)}\")\n",
    "        print(\"\\nThis may take 30 seconds to 2 minutes...\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Compile\n",
    "            compile_cmd = [\n",
    "                'javac',\n",
    "                '-cp', self.jar_path,\n",
    "                java_file\n",
    "            ]\n",
    "            subprocess.run(compile_cmd, check=True, capture_output=True)\n",
    "            \n",
    "            # Run\n",
    "            run_cmd = [\n",
    "                'java',\n",
    "                '-Xmx48g',  # Use 48GB heap\n",
    "                '-cp', f'{self.jar_path}:/content',\n",
    "                'DiscoveryRunner'\n",
    "            ]\n",
    "            result = subprocess.run(run_cmd, capture_output=True, text=True, timeout=300)\n",
    "            \n",
    "            if result.returncode != 0:\n",
    "                print(\"Error output:\")\n",
    "                print(result.stderr)\n",
    "                raise Exception(f\"Discovery failed with return code {result.returncode}\")\n",
    "            \n",
    "            # Parse output\n",
    "            print(result.stdout)\n",
    "            \n",
    "            # Read results CSV\n",
    "            results_file = '/content/results.csv'\n",
    "            if os.path.exists(results_file):\n",
    "                df = pd.read_csv(results_file)\n",
    "                return df\n",
    "            else:\n",
    "                print(\"No results file generated\")\n",
    "                return pd.DataFrame(columns=['domain', 'connections', 'percentage'])\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            raise Exception(\"Discovery timed out (>5 minutes). Try fewer seed domains.\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Discovery error: {str(e)}\")\n",
    "    \n",
    "    def _generate_discovery_java_code(self, seeds_file: str, min_conn: int, direction: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate Java code that uses cc-webgraph to run discovery.\n",
    "        \n",
    "        For backlinks: Load the transpose graph (-t suffix). In the transpose graph,\n",
    "        an edge A->B means B links to A in the original graph. So successors in the\n",
    "        transpose graph gives us the predecessors (backlinks) in the original graph.\n",
    "        \n",
    "        For outlinks: Load the regular graph. Successors gives us outgoing links.\n",
    "        \"\"\"\n",
    "        # Choose the appropriate graph file based on direction\n",
    "        if direction == 'backlinks':\n",
    "            # Use transpose graph: successors in transpose = predecessors in original\n",
    "            graph_path = f\"{self.graph_base}-t\"\n",
    "        else:\n",
    "            # Use regular graph: successors = outlinks\n",
    "            graph_path = self.graph_base\n",
    "        \n",
    "        return f'''import it.unimi.dsi.webgraph.*;\n",
    "import it.unimi.dsi.fastutil.longs.*;\n",
    "import java.io.*;\n",
    "import java.util.*;\n",
    "import java.util.zip.*;\n",
    "\n",
    "public class DiscoveryRunner {{\n",
    "    public static void main(String[] args) throws Exception {{\n",
    "        // Load the appropriate graph\n",
    "        // For backlinks: transpose graph (successors = predecessors)\n",
    "        // For outlinks: regular graph (successors = outlinks)\n",
    "        System.out.println(\"Loading graph from: {graph_path}\");\n",
    "        BVGraph graph = BVGraph.load(\"{graph_path}\");\n",
    "        System.out.println(\"Graph loaded: \" + graph.numNodes() + \" nodes\");\n",
    "        \n",
    "        // Build domain <-> ID mappings\n",
    "        System.out.println(\"Loading domain mappings...\");\n",
    "        Map<String, Integer> domainToId = new HashMap<>();\n",
    "        Map<Integer, String> idToDomain = new HashMap<>();\n",
    "        \n",
    "        try (BufferedReader br = new BufferedReader(\n",
    "                new InputStreamReader(\n",
    "                    new GZIPInputStream(\n",
    "                        new FileInputStream(\"{self.vertices_file}\"))))) {{\n",
    "            String line;\n",
    "            while ((line = br.readLine()) != null) {{\n",
    "                String[] parts = line.split(\"\\\\t\");\n",
    "                if (parts.length >= 2) {{\n",
    "                    int id = Integer.parseInt(parts[0]);\n",
    "                    String revDomain = parts[1];\n",
    "                    \n",
    "                    // Convert reversed domain (com.example) to normal (example.com)\n",
    "                    String[] domainParts = revDomain.split(\"\\\\\\\\.\");\n",
    "                    StringBuilder sb = new StringBuilder();\n",
    "                    for (int i = domainParts.length - 1; i >= 0; i--) {{\n",
    "                        if (sb.length() > 0) sb.append(\".\");\n",
    "                        sb.append(domainParts[i]);\n",
    "                    }}\n",
    "                    String domain = sb.toString();\n",
    "                    \n",
    "                    domainToId.put(domain, id);\n",
    "                    idToDomain.put(id, domain);\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        System.out.println(\"Loaded \" + domainToId.size() + \" domain mappings\");\n",
    "        \n",
    "        // Load seed domains\n",
    "        System.out.println(\"Loading seed domains...\");\n",
    "        Set<Integer> seedIds = new HashSet<>();\n",
    "        try (BufferedReader br = new BufferedReader(new FileReader(\"{seeds_file}\"))) {{\n",
    "            String line;\n",
    "            while ((line = br.readLine()) != null) {{\n",
    "                String domain = line.trim().toLowerCase();\n",
    "                Integer id = domainToId.get(domain);\n",
    "                if (id != null) {{\n",
    "                    seedIds.add(id);\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        System.out.println(\"Found \" + seedIds.size() + \" seed domains in graph\");\n",
    "        \n",
    "        if (seedIds.isEmpty()) {{\n",
    "            System.out.println(\"No valid seed domains found!\");\n",
    "            return;\n",
    "        }}\n",
    "        \n",
    "        // Run discovery: find all neighbors of seed nodes\n",
    "        System.out.println(\"Running discovery ({direction})...\");\n",
    "        Map<Integer, Integer> candidateCounts = new HashMap<>();\n",
    "        \n",
    "        for (Integer seedId : seedIds) {{\n",
    "            // Get neighbors using successors()\n",
    "            // In transpose graph: successors = who links TO this node (backlinks)\n",
    "            // In regular graph: successors = who this node links TO (outlinks)\n",
    "            LazyIntIterator neighbors = graph.successors(seedId);\n",
    "            int neighbor;\n",
    "            while ((neighbor = neighbors.nextInt()) != -1) {{\n",
    "                // Don't count seeds themselves\n",
    "                if (!seedIds.contains(neighbor)) {{\n",
    "                    candidateCounts.merge(neighbor, 1, Integer::sum);\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        System.out.println(\"Found \" + candidateCounts.size() + \" unique candidate domains\");\n",
    "        \n",
    "        // Filter by minimum connection threshold\n",
    "        System.out.println(\"Filtering by threshold >= {min_conn}...\");\n",
    "        List<Map.Entry<Integer, Integer>> results = new ArrayList<>();\n",
    "        for (Map.Entry<Integer, Integer> entry : candidateCounts.entrySet()) {{\n",
    "            if (entry.getValue() >= {min_conn}) {{\n",
    "                results.add(entry);\n",
    "            }}\n",
    "        }}\n",
    "        \n",
    "        // Sort by connection count descending\n",
    "        results.sort((a, b) -> b.getValue() - a.getValue());\n",
    "        System.out.println(\"Found \" + results.size() + \" domains meeting threshold\");\n",
    "        \n",
    "        // Write results to CSV\n",
    "        try (PrintWriter pw = new PrintWriter(new FileWriter(\"/content/results.csv\"))) {{\n",
    "            pw.println(\"domain,connections,percentage\");\n",
    "            for (Map.Entry<Integer, Integer> entry : results) {{\n",
    "                String domain = idToDomain.get(entry.getKey());\n",
    "                if (domain != null) {{\n",
    "                    int connections = entry.getValue();\n",
    "                    double percentage = (connections * 100.0) / seedIds.size();\n",
    "                    pw.printf(\"%s,%d,%.2f%n\", domain, connections, percentage);\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        \n",
    "        System.out.println(\"‚úÖ Discovery complete. Results written to /content/results.csv\");\n",
    "    }}\n",
    "}}\n",
    "'''\n",
    "\n",
    "# Initialize discovery object\n",
    "VERSION = \"cc-main-2025-26-nov-dec-jan\"\n",
    "discovery = WebgraphDiscovery(WEBGRAPH_DIR, VERSION)\n",
    "\n",
    "print(\"‚úÖ Discovery tools initialized\")\n",
    "print(f\"Graph location: {WEBGRAPH_DIR}\")\n",
    "print(f\"Version: {VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Discovery Interface üéØ\n",
    "\n",
    "### Use this form to discover related domains!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, FileLink, clear_output\n",
    "import pandas as pd\n",
    "\n",
    "# Create input widgets\n",
    "domains_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Enter seed domains, one per line:\\nexample.com\\ntest.org\\nsample.net',\n",
    "    description='',\n",
    "    layout=widgets.Layout(width='80%', height='200px'),\n",
    "    style={'description_width': '0px'}\n",
    ")\n",
    "\n",
    "min_conn_slider = widgets.IntSlider(\n",
    "    value=5,\n",
    "    min=1,\n",
    "    max=100,\n",
    "    step=1,\n",
    "    description='Min Connections:',\n",
    "    style={'description_width': '150px'},\n",
    "    layout=widgets.Layout(width='60%')\n",
    ")\n",
    "\n",
    "direction_radio = widgets.RadioButtons(\n",
    "    options=[\n",
    "        ('Backlinks (who links TO seeds)', 'backlinks'),\n",
    "        ('Outlinks (who seeds link TO)', 'outlinks')\n",
    "    ],\n",
    "    value='backlinks',\n",
    "    description='Direction:',\n",
    "    style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(\n",
    "    description='üîç Run Discovery',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px', height='40px'),\n",
    "    tooltip='Click to discover related domains'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Display form\n",
    "display(HTML(\"<h2>üìù Discovery Configuration</h2>\"))\n",
    "display(HTML(\"<p><strong>Seed Domains</strong> (one per line):</p>\"))\n",
    "display(domains_input)\n",
    "display(HTML(\"<br>\"))\n",
    "display(min_conn_slider)\n",
    "display(HTML(\"<br>\"))\n",
    "display(direction_radio)\n",
    "display(HTML(\"<br>\"))\n",
    "display(run_button)\n",
    "display(HTML(\"<hr>\"))\n",
    "display(output_area)\n",
    "\n",
    "# Button click handler\n",
    "def on_run_click(b):\n",
    "    output_area.clear_output()\n",
    "    \n",
    "    with output_area:\n",
    "        display(HTML(\"<h3>‚è≥ Processing...</h3>\"))\n",
    "        \n",
    "        # Validate input\n",
    "        domains_text = domains_input.value.strip()\n",
    "        if not domains_text:\n",
    "            print(\"‚ùå Error: Please enter at least one domain\")\n",
    "            return\n",
    "        \n",
    "        seed_domains = [d.strip() for d in domains_text.split('\\n') if d.strip()]\n",
    "        \n",
    "        if len(seed_domains) == 0:\n",
    "            print(\"‚ùå Error: Please enter at least one domain\")\n",
    "            return\n",
    "        \n",
    "        if len(seed_domains) > 1000:\n",
    "            print(\"‚ùå Error: Maximum 1000 domains allowed\")\n",
    "            print(f\"You entered: {len(seed_domains)} domains\")\n",
    "            return\n",
    "        \n",
    "        # Validate seeds exist in webgraph\n",
    "        print(f\"Validating {len(seed_domains)} seed domains...\")\n",
    "        found, not_found = discovery.validate_seeds(seed_domains)\n",
    "        \n",
    "        if len(found) == 0:\n",
    "            print(\"\\n‚ùå Error: None of the seed domains were found in the webgraph\")\n",
    "            print(\"\\nDomains not found:\")\n",
    "            for d in not_found[:10]:\n",
    "                print(f\"  ‚Ä¢ {d}\")\n",
    "            if len(not_found) > 10:\n",
    "                print(f\"  ... and {len(not_found)-10} more\")\n",
    "            return\n",
    "        \n",
    "        if len(not_found) > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è Warning: {len(not_found)} domains not found in webgraph:\")\n",
    "            for d in not_found[:5]:\n",
    "                print(f\"  ‚Ä¢ {d}\")\n",
    "            if len(not_found) > 5:\n",
    "                print(f\"  ... and {len(not_found)-5} more\")\n",
    "            print(f\"\\nProceeding with {len(found)} valid domains\\n\")\n",
    "        else:\n",
    "            print(f\"‚úÖ All {len(found)} seed domains found in webgraph\\n\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(f\"Configuration:\")\n",
    "        print(f\"  ‚Ä¢ Direction: {direction_radio.value}\")\n",
    "        print(f\"  ‚Ä¢ Minimum connections: {min_conn_slider.value}\")\n",
    "        print(f\"  ‚Ä¢ Valid seed domains: {len(found)}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            # Run discovery\n",
    "            results_df = discovery.discover(\n",
    "                seed_domains=found,\n",
    "                min_connections=min_conn_slider.value,\n",
    "                direction=direction_radio.value\n",
    "            )\n",
    "            \n",
    "            # Clear processing message\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Display results\n",
    "            if len(results_df) == 0:\n",
    "                display(HTML(\"<h3>‚ùå No Results Found</h3>\"))\n",
    "                print(\"No domains found matching the criteria.\")\n",
    "                print(\"\\nTry:\")\n",
    "                print(\"  ‚Ä¢ Lowering the minimum connections threshold\")\n",
    "                print(\"  ‚Ä¢ Using different seed domains\")\n",
    "                print(\"  ‚Ä¢ Switching between backlinks and outlinks\")\n",
    "            else:\n",
    "                display(HTML(f\"<h3>‚úÖ Found {len(results_df):,} Domains</h3>\"))\n",
    "                print(f\"Discovered {len(results_df):,} domains with ‚â•{min_conn_slider.value} connections\\n\")\n",
    "                \n",
    "                # Style and display dataframe\n",
    "                display(HTML(\"<h4>Top Results:</h4>\"))\n",
    "                \n",
    "                styled_df = results_df.head(100).style.format({\n",
    "                    'connections': '{:,.0f}',\n",
    "                    'percentage': '{:.2f}%'\n",
    "                }).background_gradient(subset=['connections'], cmap='YlOrRd')\n",
    "                \n",
    "                display(styled_df)\n",
    "                \n",
    "                if len(results_df) > 100:\n",
    "                    print(f\"\\n(Showing top 100 of {len(results_df):,} results. Download CSV for full list.)\")\n",
    "                \n",
    "                # Summary statistics\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"Summary Statistics:\")\n",
    "                print(f\"  ‚Ä¢ Total discovered: {len(results_df):,} domains\")\n",
    "                print(f\"  ‚Ä¢ Connections range: {results_df['connections'].min():.0f} - {results_df['connections'].max():.0f}\")\n",
    "                print(f\"  ‚Ä¢ Mean connections: {results_df['connections'].mean():.1f}\")\n",
    "                print(f\"  ‚Ä¢ Median connections: {results_df['connections'].median():.0f}\")\n",
    "                print(\"=\"*60)\n",
    "                \n",
    "                # Download link\n",
    "                display(HTML(\"<br><h4>üíæ Download Full Results</h4>\"))\n",
    "                display(FileLink('/content/results.csv', result_html_prefix=\"üì• Click to download: \"))\n",
    "                print(f\"\\nCSV contains all {len(results_df):,} discovered domains\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(\"<h3>‚ùå Error During Discovery</h3>\"))\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            print(\"\\nüìù Troubleshooting:\")\n",
    "            print(\"1. Check that all setup cells completed successfully\")\n",
    "            print(\"2. Verify you're using High-RAM runtime\")\n",
    "            print(\"3. Try restarting runtime: Runtime ‚Üí Restart runtime\")\n",
    "            print(\"4. Try with fewer seed domains\")\n",
    "\n",
    "run_button.on_click(on_run_click)\n",
    "\n",
    "print(\"\\nüí° Tip: Start with 10-20 seed domains and min_connections=5 for fast results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Citation & References\n",
    "\n",
    "If you use this notebook in your research, please cite:\n",
    "\n",
    "```bibtex\n",
    "@article{carragher2024detection,\n",
    "  title={Detection and Discovery of Misinformation Sources using Attributed Webgraphs},\n",
    "  author={Carragher, Peter and Williams, Evan M and Carley, Kathleen M},\n",
    "  journal={Proceedings of the International AAAI Conference on Web and Social Media},\n",
    "  volume={18},\n",
    "  pages={218--229},\n",
    "  year={2024},\n",
    "  url={https://arxiv.org/abs/2401.02379}\n",
    "}\n",
    "\n",
    "@article{carragher2025misinformation,\n",
    "  title={Misinformation Resilient Search Rankings with Attributed Webgraphs},\n",
    "  author={Carragher, Peter and Williams, Evan M and Spezzano, Francesca and Carley, Kathleen M},\n",
    "  journal={ACM Transactions on Intelligent Systems and Technology},\n",
    "  year={2025}\n",
    "}\n",
    "```\n",
    "\n",
    "**Links:**\n",
    "- Paper (ICWSM 2024): https://arxiv.org/abs/2401.02379\n",
    "- GitHub Repository: https://github.com/CASOS-IDeaS-CMU/Detection-and-Discovery-of-Misinformation-Sources\n",
    "- CommonCrawl Webgraphs: https://commoncrawl.org/web-graphs\n",
    "- cc-webgraph Tools: https://github.com/commoncrawl/cc-webgraph\n",
    "\n",
    "**Contact:**\n",
    "- Peter Carragher: pcarragh@andrew.cmu.edu\n",
    "- CASOS Lab: http://casos.cs.cmu.edu/\n",
    "\n",
    "---\n",
    "\n",
    "**License:** MIT\n",
    "\n",
    "**Acknowledgments:** This notebook uses the CommonCrawl web graph dataset and the WebGraph framework developed by Sebastiano Vigna and Paolo Boldi."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
